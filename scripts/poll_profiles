#!/usr/bin/env python
#
# Usage: python poll_profiles.py <profiles-glob> <enrichment-service-URI>

import sys, os, glob, fnmatch
import argparse
import base64
import datetime, time
from itertools import groupby
from urllib import urlencode
from amara.thirdparty import json, httplib2
from amara.lib.iri import is_absolute, join
from amara import bindery
import xmltodict
from math import floor

# FIXME Turns out this isn't always correct. Sometimes Series files are located in
# different directories than its Item files. No clear deterministic path between
# them, so will have to find the file instead using globbing
#ARC_RELATED_FILE = lambda srcdir, htype, hid: os.path.join(os.path.dirname(srcdir+os.sep),"%s_%s.xml"%(htype.replace(' ',''),hid))
ARC_RELATED_FILE = lambda srcdir, htype, hid: os.path.join(os.sep.join(os.path.dirname(srcdir+os.sep).split(os.sep)[:-1]),"*","%s_%s.xml"%(htype.replace(' ',''),hid))

URI_BASE = None
ENRICH = "/enrich" # enrichment service URI
H = httplib2.Http('/tmp/.pollcache')
H.force_exception_as_status_code = True

def process_profile(uri_base, profile_f):
    global URI_BASE, ENRICH

    with open(profile_f, 'r') as fprof:
        try:
            profile = json.load(fprof)
        except Exception as e:
            print >> sys.stderr, 'Error reading source profile.'
            print >> sys.stderr, "Detailed error information:"
            print >> sys.stderr, e
            return False

    # Pause in secs between collection ingests
    sleep = profile.get(u'sleep', 0)

    URI_BASE = uri_base
    if not is_absolute(ENRICH):
        ENRICH = URI_BASE + ENRICH

    getRecord = profile.get(u'get_record', None)
    subResources = profile.get(u'subresources')
    blacklist = profile.get(u'blacklist',[])
    ptype = profile.get(u'type').lower()
    if getRecord:
        process = TYPE_PROCESSORS.get((ptype,'rec'))
        process(profile)
    elif not subResources: # i.e. all subresources
        process = TYPE_PROCESSORS.get((ptype,'all'))
        process(profile,blacklist)
    else:
        process = TYPE_PROCESSORS.get((ptype,'coll'))
        if not process:
            print >> sys.stderr, "The ingest of individual %s collections is not supported at this time"%(ptype.upper())
            sys.exit(1)

        for subr in subResources:
            process(profile, subr)
            time.sleep(sleep)

    return True

def process_primo_all(profile, blacklist=None):
    # TODO flag to stop requesting
    request_more = True
    index = 0
    while request_more:
        collection = {}
        collection['id'] = "1"
        collection['title'] = "mwdl"
        collection['items'] = []
        endpoint = "%s&bulkSize=%s&indx=%s" % (profile[u'endpoint_URL'], profile[u'bulk_size'], index)

        resp, content = H.request(endpoint)
        if not resp[u'status'].startswith('2'):
            print >> sys.stderr, ' HTTP error (%s) resolving URL: %s' % (resp[u'status'], endpoint)
            request_more = False

        endpoint_content = ARC_PARSE(content)
        total_hits = endpoint_content['SEGMENTS']['JAGROOT']['RESULT']['DOCSET']['TOTALHITS']
        print >> sys.stderr, "%s of %s total documents" % (index, total_hits)
        items = endpoint_content['SEGMENTS']['JAGROOT']['RESULT']['DOCSET']['DOC']

        for item in (items if isinstance(items, list) else [items]):
            item['_id'] = item['PrimoNMBib']['record']['control']['recordid']
            collection['items'].append(item)
            index += 1
        enrich_coll(profile, collection['id'], json.dumps(collection))

        if int(index) >= int(total_hits):
            request_more = False

    return True

ARC_PARSE = lambda doc: xmltodict.parse(doc,xml_attribs=True,attr_prefix='',force_cdata=False,ignore_whitespace_cdata=True)

#def skip_cdata(path,key,data):
#    if '#text' in data:
#        del data['#text']
#    return key, data
#
#ARC_PARSE = lambda doc: xmltodict.parse(doc,xml_attribs=True,attr_prefix='',postprocessor=skip_cdata)
def process_arc_all(profile,blacklist=None):
    src_URL = profile.get('endpoint_URL')
    assert src_URL.startswith('file:/') # assumes no authority and the non-broken use of //
    src_dir = src_URL[5:]

    collections = {} 
    # Create temp dir
    cache_dir = create_temp_dir("ingest_nara")
    print "Walking directory: "+src_dir
    for (root, dirs, files) in os.walk(src_dir):
        for filename in fnmatch.filter(files, 'Item_*.xml'):
            item_fn = os.path.join(root,filename)
            item_f = open(item_fn,'r')
            item = ARC_PARSE(item_f)['archival-description']
            item_f.close()

            # set our generic identifier property
            item['_id'] = item['arc-id']

            hier_items = item['hierarchy']['hierarchy-item']
            for hi in (hier_items if isinstance(hier_items,list) else [hier_items]):
                htype = hi['hierarchy-item-lod']
                # Record Group mapped to collection
                if not htype.lower() == 'record group': continue

                hid = hi['hierarchy-item-id']

                if hid not in collections:
                    # Grab series information from item
                    coll = {}
                    coll['id'] = hid
                    coll['title'] = hi['hierarchy-item-title']
                    coll['items'] = []
                    collections[hid] = coll
                else:
                    coll = collections[hid]

                coll_fn = os.path.join(cache_dir, "coll_%s" % coll['id'])
                coll_f = open(coll_fn, 'a')
                coll_f.write(str(item) + "\n")
                coll_f.close()

    limit = 1000
    for cid in collections:
        # Open tmp collection file and append items
        coll_fn = os.path.join(cache_dir, "coll_%s" % cid)
        coll_f = open(coll_fn, 'r')
        lines = coll_f.readlines()
        coll_f.close()
        os.remove(coll_fn)

        i = 0
        for line in lines:
            collections[cid]['items'].append(eval(line))
            i += 1

            if i == limit or line == lines[-1]:
                print >> sys.stderr, "Enriching collection %s" % cid
                enrich_coll(profile,cid,json.dumps(collections[cid]))
                del collections[cid]['items'][:]
                i = 0   

        del collections[cid]['items']

def enrich_coll(profile, subr, content):
    # Enrich retrieved data
    global ENRICH
    
    headers = {
        "Content-Type": "application/json",
        "Pipeline-Coll": ','.join(profile["enrichments_coll"]),
        "Pipeline-Rec": ','.join(profile["enrichments_rec"]),
        "Source": profile['name'],
        "Contributor": base64.b64encode(json.dumps(profile.get(u'contributor',{})))
    }
    if subr:
        headers["Collection"] = subr

    resp, content = H.request(ENRICH, 'POST' ,body=content, headers=headers)
    if not str(resp.status).startswith('2'):
        print >> sys.stderr, '  HTTP error with enrichment service: '+repr(resp)

def process_oai_rec(profile):
    endpoint = profile[u'get_record']
    if not is_absolute(endpoint):
        endpoint = URI_BASE + endpoint
    print >> sys.stderr, endpoint

    resp, content = H.request(endpoint)
    if not str(resp.status).startswith('2'):
        print >> sys.stderr, '  HTTP error ('+resp[u'status']+') resolving URL: ' + endpoint
        return False

    subr = profile[u'name']
    enrich_coll(profile,subr,content)

def process_oai_coll(profile,subr):
    # For now, a simplifying assumption that string concatenation produces a
    # full URI from the combination of the endpoint URL and each subresource id.
    # Better might be a single field listing all URIs but unclear how that extends
    # to other protocols.

    # If multiple requests are required to harvest all information from a resource, they will
    # give us 'resumption tokens' after each request until we are done. Passing the resumption
    # token will provide the next batch of results
    global URI_BASE

    request_more, resumption_token = True, ""
    while request_more:
        endpoint = profile[u'endpoint_URL'] + (subr if subr != profile[u'name'] else "")
        if not is_absolute(endpoint):
            endpoint = URI_BASE + endpoint
        if resumption_token:
            endpoint += '&' + urlencode({'resumption_token': resumption_token})
        print >> sys.stderr, endpoint

        resp, content = H.request(endpoint)
        if not resp[u'status'].startswith('2'):
            print >> sys.stderr, '  HTTP error ('+resp[u'status']+') resolving URL: ' + endpoint
            continue
        endpoint_content = json.loads(content)
        resumption_token = endpoint_content['resumption_token']

        content = json.dumps(endpoint_content)
        enrich_coll(profile,subr,content)

        request_more = resumption_token is not None and len(resumption_token) > 0

def process_oai_all(profile,blacklist=[]):
    # Get all sets
    global URI_BASE
    url = profile[u'list_sets']
    if not is_absolute(url):
        url = URI_BASE + url
    resp, content = H.request(url)
    if not resp[u'status'].startswith('2'):
        print >> sys.stderr, ' HTTP error ('+resp[u'status']+') resolving URL: ' + url
        return False

    sleep = profile.get(u'sleep',0)
    
    subResources = []
    if len(content) > 2:
        set_content = json.loads(content)
        for s in set_content:
            if s[u'setSpec']:
                subResources.append(s[u'setSpec'])
    else:
        # Case where provider does not support Sets
        subResources.append(profile['name'])

    # Process the sets
    subr_to_process =[subr for subr in subResources if subr not in blacklist]
    for subr in subr_to_process:
        process_oai_coll(profile,subr)
        time.sleep(sleep)


def get_current_username():
    """Returns the name of the current user."""
    import os
    import pwd
    return pwd.getpwuid( os.getuid() )[ 0 ]


def create_temp_dir(operation=""):
    """Returns a new temp dir.
    
    The temp dir is created using current user name and provided operation.
    """
    import tempfile
    prefix = "%s_%s" % (get_current_username(), operation)
    return tempfile.mkdtemp(prefix=prefix)


def normalize_collection_name(collection_name):
    """Removes bad characters from collection names, to have safe filenames."""
    import re
    x = re.sub(r'[/() \t]', r'_', collection_name)
    x = re.sub(r'_+', r'_', x)
    x = re.sub(r':', r'_', x)
    return x.lower()


# Used for Smithsonian data
collections = {} 

def process_edan_all(profile, blacklist=None):
    src_URL = profile.get('endpoint_URL')
    assert src_URL.startswith('file:/') # assumes no authority and the non-broken use of //
    src_dir = src_URL[5:]
    
    global collections
    collections = {}
    cache_dir = create_temp_dir("ingest_edan")
    print "Using cache dir: " + cache_dir
    print "Walking directory: " + src_dir

    def cache_file_name(cache_dir, collection):
        f = os.path.join(cache_dir, "coll_" + normalize_collection_name(collection))
        return f

    def handle_document(_, item):
        global collections

        desc_non = item["descriptiveNonRepeating"]
        item["_id"] = desc_non["record_ID"]

        freetext = item["freetext"]

        if not "setName" in freetext: # So there is no collection
            return True #XML parser need to get True here to continue parsing

        colls = freetext["setName"]
        it = colls
        if not isinstance(colls, list):
            it = [colls]

        for c in it: 
            if not "#text" in c:
                continue
            
            hid = normalize_collection_name(c["#text"])
            htitle = c["#text"]
            
            if hid not in collections:
                # Grab series information from item
                coll = {}
                coll['id'] = hid
                coll['title'] = htitle
                coll['items'] = []
                collections[hid] = coll
            else:
                coll = collections[hid]

            # Create tmp file to hold collections items
            coll_fn = cache_file_name(cache_dir, coll['id'])
            coll_f = open(coll_fn,'a')
            coll_f.write(str(item)+"\n")
            coll_f.close()

        return True

    for (root, dirs, files) in os.walk(src_dir):
        for filename in fnmatch.filter(files, '*_DPLA.xml'):
            item_fn = os.path.join(root,filename)
            print "Processing file: " + item_fn
            try:
                item_f = open(item_fn,'r')
                xmltodict.parse(item_f, item_depth=3, item_callback=handle_document)
            except Exception as e:
                print >> sys.stderr, '[ERROR]', e.message 
            else:
                item_f.close()
 
    limit = 1000
    for cid in collections:
        # Open tmp collection file and append items
        coll_fn = cache_file_name(cache_dir, cid)
        coll_f = open(coll_fn, 'r')

        i = 0
        step = 0
        for line in coll_f:
            collections[cid]['items'].append(eval(line))
            i += 1

            if i == limit:
                print >> sys.stderr, "Enriching collection [%s]" % cid
                enrich_coll(profile,cid,json.dumps(collections[cid]))
                del collections[cid]['items'][:]
                i = 0

        if collections[cid]['items']:
            print >> sys.stderr, "Enriching collection [%s]" % cid
            enrich_coll(profile,cid,json.dumps(collections[cid]))
            del collections[cid]['items'][:]

        del collections[cid]['items']
        coll_f.close()

    print >> sys.stderr, "Removing cache dir [%s]" % cache_dir
    os.system("rm -rf " + cache_dir)

def process_mets_coll(profile, subr):

    def document_links(document_list_uri):
        print >> sys.stderr, "Enriching collection %s" % subr
        print >> sys.stderr, document_list_uri
        resp, content = H.request(document_list_uri)
        if not resp[u'status'].startswith('2'):
            print >> sys.stderr, ' HTTP error (%s) resolving URL: %s' % (resp[u'status'], endpoint)
            return
        endpoint_content = ARC_PARSE(content)
        for d in endpoint_content["mets:mets"]:
            if "mets:dmdSec" in d:
                records = endpoint_content["mets:mets"][d]
                for record in records:
                    if not record["ID"].startswith("collection-description-mods"):
                        yield record["mets:mdRef"]["xlink:href"]

    global URI_BASE
    endpoint = profile[u'endpoint_URL'].format(subr)

    items = []
    count = 0
    total_count = 0
    for doc_url in document_links(endpoint):
        resp, content = H.request(doc_url)
        if not resp[u'status'].startswith('2'):
            print >> sys.stderr, ' HTTP error (%s) resolving URL: %s' % (resp[u'status'], endpoint)
            return False
        doc_content = ARC_PARSE(content)
        if "mods" in doc_content:
            item = doc_content["mods"]
            for _id_dict in item["identifier"]:
                if _id_dict["type"] == "uri":
                    item["_id"] = _id_dict["#text"]
                    items.append(item)
                    count += 1
                    total_count += 1
        if count >= 100:
            enrich_coll(profile, subr, json.dumps({"items": items}))
            print >> sys.stderr, "%d documents processed" % total_count
            count = 0
            items = []
    if items:
        enrich_coll(profile, subr, json.dumps({"items": items}))
        print >> sys.stderr, "%d documents processed" % total_count

def process_nypl_coll(profile, subr):
    endpoint = profile[u'endpoint_URL'].format(subr)
    doc_endpoint = profile["get_record_URL"]
    args = {"per_page": 100, "page": 1}
    print >> sys.stderr, "Enriching collection %s" % subr
    done = False
    page = 1
    while not done:
        args["page"] = page
        request_url = endpoint + "?" + urlencode(args)
        print >> sys.stderr, request_url
        resp, content = H.request(request_url)
        if not resp[u'status'].startswith('2'):
            print >> sys.stderr, ' HTTP error (%s) resolving URL: %s' % (resp[u'status'], endpoint)
            return False
        response_dict = ARC_PARSE(content)["nyplAPI"]
        success = response_dict["response"]["headers"]["code"] == "200"
        if success:
            total_pages =  response_dict["request"]["totalPages"]
            current_page = response_dict["request"]["page"]
            done = total_pages == current_page
            page += 1
            items = []
            for item_dict in response_dict["response"]["capture"]:
                doc_url = doc_endpoint.format(item_dict["uuid"])
                resp, content = H.request(doc_url)
                if not resp[u'status'].startswith('2'):
                    print >> sys.stderr, ' HTTP error (%s) resolving URL: %s' % (resp[u'status'], endpoint)
                    return False
                doc_response = ARC_PARSE(content)["nyplAPI"]
                assert doc_response["response"]["headers"]["code"] == "200", doc_response["response"]["headers"]["message"]
                item = doc_response["response"]["mods"]
                item["_id"] = item_dict["uuid"]
                item["tmp_image_id"] = item_dict["imageID"]
                item["tmp_high_res_link"] = item_dict["highResLink"] if "highResLink" in item_dict else None
                items.append(item)
            if items:
                enrich_coll(profile, subr, json.dumps({"items": items}))
        else:
            raise Exception(response_dict["response"]["headers"]["message"])

def process_nypl_all(profile, blacklist=tuple()):
    # Get all sets
    global URI_BASE
    url = profile[u'list_sets']
    if not is_absolute(url):
        url = URI_BASE + url
    resp, content = H.request(url)
    if not resp[u'status'].startswith('2'):
        print >> sys.stderr, ' HTTP error ('+resp[u'status']+') resolving URL: ' + url
        return False

    sleep = profile.get(u'sleep', 0)

    subResources = []
    response = ARC_PARSE(content)["nyplAPI"]["response"]
    for r in response:
        if "collection" == r:
            for coll_dict in response[r]:
                if "uuid" in coll_dict:
                    subResources.append(coll_dict["uuid"])

    # Process the sets
    subr_to_process = (subr for subr in subResources if subr not in blacklist)
    for subr in subr_to_process:
        process_nypl_coll(profile, subr)
        time.sleep(sleep)

def process_ia_coll(profile, subr):

    def get_parsed_xml(url):
        resp, content = H.request(url)
        if not resp[u'status'].startswith('2'):
            error = ' HTTP error (%s) resolving URL: %s' % (resp[u'status'], endpoint)
            print >> sys.stderr, error
            raise Exception(error)
        return ARC_PARSE(content)

    endpoint = profile[u'endpoint_URL'].format(subr)
    args = {"rows": 100, "page": 1}
    print >> sys.stderr, "Enriching collection %s" % subr
    done = False
    page = 1
    while not done:
        args["page"] = page
        request_url = endpoint + "&" + urlencode(args)
        print >> sys.stderr, request_url
        resp, content = H.request(request_url)
        if not resp[u'status'].startswith('2'):
            print >> sys.stderr, ' HTTP error (%s) resolving URL: %s' % (resp[u'status'], endpoint)
            return False
        response_dict = json.loads(content)["response"]
        total_docs = int(response_dict["numFound"])
        read_docs = int(response_dict["start"])
        done = (total_docs - read_docs) < args["rows"]
        page += 1
        items = []
        for item_dict in response_dict["docs"]:
            identifier = item_dict["identifier"]
            file_url_pattern = profile["get_file_URL"]
            files_url = file_url_pattern.format(identifier, profile["prefix_files"].format(identifier))
            files_response = get_parsed_xml(files_url)["files"]
            item_data = {"dc": None, "meta": None, "gif": None, "pdf": None,
                         "shown_at": profile["shown_at_URL"].format(identifier),
                         "marc": None}
            for file_info in files_response["file"]:
                format = file_info["format"]
                name = file_info["name"]
                if format == "Text PDF":
                    item_data["pdf"] = name
                elif format == "Animated GIF":
                    item_data["gif"] = name
                elif format == "Grayscale LuraTech PDF" and not item_data["pdf"]:
                    item_data["pdf"] = name
                elif format == "Metadata" and name.endswith("_meta.xml"):
                    item_data["meta"] = name
                elif format == "Dublin Core":
                    item_data["dc"] = name
                elif format == "MARC":
                    item_data["marc"]= name

            assert item_data["meta"] is not None, "document \"" + identifier + "\" meta data is absent"

            item = {"files": item_data,
                    "_id": identifier}
            item.update(get_parsed_xml(file_url_pattern.format(identifier, item_data["meta"])))
            if item_data["dc"]:
                item.update(get_parsed_xml(file_url_pattern.format(identifier, item_data["dc"])))
            items.append(item)
        if items:
            enrich_coll(profile, subr, json.dumps({"items": items}))


TYPE_PROCESSORS = {
    ('arc','coll'): None,
    ('arc','all'): process_arc_all,
    ('oai','coll'): process_oai_coll,
    ('oai','all'): process_oai_all,
    ('edan','coll'): None,
    ('edan','all'): process_edan_all,
    ('oai','rec'): process_oai_rec,
    ('primo','coll'): None,
    ('primo','all'): process_primo_all,
    ('mets', 'coll'): process_mets_coll,
    ('mets', 'all'): None,
    ('nypl', 'coll'): process_nypl_coll,
    ('nypl', 'all'): process_nypl_all,
    ('ia', 'coll'): process_ia_coll,
    ('ia', 'all'): None,
}

def define_arguments():
    """
    Defines command line arguments for the current script
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("uri_base", help="The base URI for the server hosting the enrichment pipeline")
    parser.add_argument("profile", help="The path to the profile(s) to be processed", nargs="+")
    return parser


def main(argv):
    parser = define_arguments()
    args = parser.parse_args(argv[1:])
    for profile in args.profile:
        print >> sys.stderr, 'Processing profile: '+profile
        process_profile(args.uri_base, profile)

if __name__ == '__main__':
    main(sys.argv)
